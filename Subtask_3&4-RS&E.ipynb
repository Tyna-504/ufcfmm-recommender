{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top-of-page\"></a>\n",
    "# Table of Contents #\n",
    "Click on a chapter:<br>\n",
    "**[1. Subtask 3 & 4: Recommender System and Model Evaluation](#1)<br>**\n",
    "**[a. K-Nearest Neighbours](#a)<br>**\n",
    "[a.i KNN Model 1](#ai)<br>\n",
    "[a.ii KNN Model 2](#aii)<br>\n",
    "[a.iii KNN Model 1&2 Conclusion](#aiii)<br>\n",
    "**[b. RFC (Random Forest Classifier)](#b)<br>**\n",
    "[b.i RFC Model](#bi)<br>\n",
    "[b.ii RFC Model Conclusion](#bii)<br>\n",
    "**[c. Collaborative Filtering](#c)<br>**\n",
    "[c.i Collaborative Filtering Model](#ci)<br>\n",
    "[c.ii Running the Collaborative Filtering Recommender](#cii)<br>\n",
    "[c.iii Collaborative Filtering Conclusion](#ciii)<br>\n",
    "**[d. KNN with Means Model](#d)<br>**\n",
    "[d.i KNN with Means Model (with Price)](#di)<br>\n",
    "[d.ii KNN with Means Model (with Price and Review Score)](#dii)<br>\n",
    "[d.iii KNN with Means Model - Product Recommendations for a User](#diii)<br>\n",
    "[d.iv KNN with Means Conclusion](#div)<br>\n",
    "**[2. Conclusion](#2)<br>**\n",
    "**[3. References](#3)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtasks 3 and 4 are closely intertwined, with the former involving building the recommendation system and the latter involving evaluating its effectiveness using appropriate metrics. Performing these tasks together in one notebook allows for a more seamless and integrated approach to building and evaluating the recommendation system. This also makes it easier to keep track of the steps taken and results obtained and make any necessary changes or improvements. Additionally, since these tasks are shorter compared to Subtasks 1, 2, and 5, it makes sense to keep them together in one notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Subtask 3 & 4: Recommender System and Model Evaluation\n",
    "\n",
    "For this project, we used three methods to build a recommender system: K-Nearest Neighbour, Random Forest Classifier, and Collaborative Filtering. K-Nearest Neighbour (KNN) is a simple and effective method to predict user ratings based on the similarity of items. It is computationally efficient and easy to implement, making it a popular choice for recommender systems. Random Forest Classifier (RFC) is a powerful machine learning algorithm that can handle large datasets and high-dimensional feature spaces. It works by creating multiple decision trees and combining their predictions to make a final recommendation. Collaborative Filtering (CF) is a technique that recommends items to users based on similar users preferences. CF is very effective in many recommendation scenarios and is particularly useful when dealing with sparse data. By combining these three methods, we aim to build a comprehensive and accurate recommender system that can provide users with personalised recommendations based on their past behaviour and preferences.\n",
    "\n",
    "### Training and Testing the Models ###\n",
    "To train the model, we will select features such as product price and review score and use the target variable of the product ID as the prediction output.\n",
    "\n",
    "To begin with, we will create models using KNeighborsClassifier and RandomForestClassifier. \n",
    "\n",
    "It is crucial to evaluate the effectiveness of the recommender system to ensure that it is performing well and providing accurate recommendations to users. The metrics such as precision, recall, f1, etc., are commonly used to evaluate the performance of recommendation systems.<br>\n",
    "\n",
    "However, the choice of evaluation metric should depend on the specific goals of the recommendation system and the type of data being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: surprise in /home/reece3.nicholls/.local/lib/python3.8/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /home/reece3.nicholls/.local/lib/python3.8/site-packages (from surprise) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/reece3.nicholls/.local/lib/python3.8/site-packages (from scikit-surprise->surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "!{sys.executable} -m pip install surprise\n",
    "from surprise import Dataset, Reader, KNNWithMeans, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a\"></a>\n",
    "# a) KNN (K-Nearest Neighbours) #\n",
    "<a id=\"ai\"></a>\n",
    "## a.i) KNN Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbour is a supervised clustering algorithm which groups similar data points together (LEDU, 2018). KNN is one of the more simpler models to implement, but can still produce meaningful results. Clusters of similar customers can be created, based on their data parameters we can use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared and split the data into train and test data, we can train the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the KNN model\n",
      "Precision: 0.7731484933007162\n",
      "Recall: 0.12812706816677696\n",
      "f1: 0.10101141726902235\n"
     ]
    }
   ],
   "source": [
    "# create and train the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data using the KNN model\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aii\"></a>\n",
    "### a.ii) KNN Model 2 ###\n",
    "\n",
    "To try and improve the accuracy and precision of our model, we will implement one-hot encoding for categorical data, scale the variable to normalise their values and remove bias, and use grid search to determine the best K value for KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the best KNN model\n",
      "Precision: 0.8581835761729941\n",
      "Recall: 0.28880070546737213\n",
      "f1: 0.25170699245110456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-919fb23ee9dc>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n"
     ]
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score', 'product_category_name', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']]\n",
    "y = data['product_id']\n",
    "\n",
    "# encode the categorical feature as numeric\n",
    "X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
    "\n",
    "# scale the features using standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# create a parameter grid for k\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "\n",
    "# create and train the model using grid search and cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # print the best value of k\n",
    "# print(\"Best value of k:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on the testing data using the best KNN model\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "\n",
    "print(\"Scores for the best KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a.iii\"></a>\n",
    "## a.iii) KNN Model 1&2 Conclusion\n",
    "\n",
    "In conclusion, K-Nearest Neighbour (KNN) is a practical supervised clustering algorithm for grouping similar data points. By training the model on selected features and target variables, we can create clusters of similar customers. However, to improve the accuracy and precision of the KNN model, we can use techniques like one-hot encoding for categorical data, scaling variables to normalize their values and remove bias, and grid search to determine the best K value for KNN. By applying these techniques and including more parameters for a given product, we improved our model's precision to 87%. Next, we will look at RFC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_[Go to top](#top-of-page)_\n",
    "\n",
    "<a id=\"b\"></a>\n",
    "# b) RFC (Random Forest Classifier) #\n",
    "<a id=\"b.i\"></a>\n",
    "## b.i) RFC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier is a supervised clustering algorithm for classification and regression problems (Sruthi, 2021). RFC can deal with categorical data and outliers easily which makes it a good choice for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, n_estimators=10, n_jobs=-1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, n_jobs=-1, max_depth=4, verbose=0)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the system we are running the notebook on, we had to reduce the size of the dataset in order to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: JupyterHub kernel crashes due to the size of the data, therefore we have grabbed a small sample of test data.\n",
    "X_test_rfc = X_test.sample(frac=0.3, random_state=200)\n",
    "y_test_rfc = y_test.sample(frac=0.3, random_state=200)\n",
    "\n",
    "# make predictions on the testing data using the RFC model\n",
    "y_pred_rfc = rfc.predict(X_test_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the RFC model\n",
      "Precision: 0.9694328413312084\n",
      "Recall: 0.28880070546737213\n",
      "f1: 0.25170699245110456\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test_rfc, y_pred_rfc, average=\"weighted\", zero_division=1)\n",
    "# recall = recall_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "# f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the RFC model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bii\"></a>\n",
    "## b.ii) RFC Model Conclusion\n",
    "\n",
    "To conclude, the Random Forest Classifier (RFC) is a supervised clustering algorithm that can effectively handle categorical data and outliers, making it a suitable choice for our dataset. The RFC model was trained and tested using a reduced sample of data due to system limitations, and its precision score was high. However, the suspiciously high score could indicate the overfitting of the model. However, the recall score is relatively low at 0.292, indicating that the model may have missed many potentially relevant recommendations. We decided to look into Collaborative Filtering next.\n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"c\"></a>\n",
    "# c) Collaborative Filtering\n",
    "<a id=\"ci\"></a>\n",
    "## c.i) Collaborative Filtering Model ##\n",
    "\n",
    "Collaborative Filtering is a model creating for recommending an item to a user, based off of their previous purchased items and other users purchases (Google Machine Learning, 2023). \n",
    "\n",
    "If user A buys an item, and user B has a similar purchase history, user B will be recommended the new item too.\n",
    "\n",
    "Collaborative filtering takes customers' previous orders and identifies patterns, and recommends products to customers based on previous customers' orders. If customer A orders products X and Y, then customer B who has ordered product X, will be recommended product Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "\n",
    "# filter the data to include only the most active customers\n",
    "# NOTE: this is mainly done due to performance issues.\n",
    "customer_counts = data['customer_id'].value_counts()\n",
    "active_customers = customer_counts[customer_counts > 3].index\n",
    "data = data[data['customer_id'].isin(active_customers)]\n",
    "\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# create a pivot table with customers as rows and products as columns using the training data\n",
    "pivot = train_data.pivot_table(index='customer_id', columns='product_id', values='review_score')\n",
    "\n",
    "# fill missing values with 0\n",
    "pivot = pivot.fillna(0)\n",
    "\n",
    "# convert the pivot table to a sparse matrix\n",
    "matrix = csr_matrix(pivot.values)\n",
    "\n",
    "# create and fit a NearestNeighbors model using the training data\n",
    "model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to recommend products for a given customer\n",
    "def recommend_products(customer_id):\n",
    "    # find the index of the customer in the pivot table\n",
    "    customer_index = pivot.index.get_loc(customer_id)\n",
    "    \n",
    "    # find the k nearest neighbors of the customer\n",
    "    distances, indices = model.kneighbors(pivot.iloc[customer_index, :].values.reshape(1, -1), n_neighbors=6)\n",
    "    \n",
    "    # get the product ids of the products purchased by the nearest neighbors\n",
    "    product_ids = []\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "#             product_ids.extend(pivot.index[indices.flatten()[i]])\n",
    "            # append the recommended product ID to the array.\n",
    "            product_ids.append(pivot.columns[indices.flatten()[i]])\n",
    "    \n",
    "    # return the most common product ids\n",
    "    return pd.Series(product_ids).value_counts().head().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a03e401d58a45187271718c5d7610422', '87689c3ea34514e449355126a5fc299e', '87cb507e0daa37bbf34956fd59eba832', '87d780fa7d2cf3710aa02dc4ca8db985', '872db866d615db59612ac933f43d6b22']\n"
     ]
    }
   ],
   "source": [
    "# test the recommend_products function on a customer from the testing data.\n",
    "test_customer = test_data.iloc[0]['customer_id']\n",
    "recommended_products = recommend_products(test_customer)\n",
    "print(recommended_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cii\"></a>\n",
    "## c.ii) Running the Collaborative Filtering Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing a customer ID into the recommender model, the output is a list of recommended products for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['87d780fa7d2cf3710aa02dc4ca8db985', '87cb507e0daa37bbf34956fd59eba832', '87590844d536e6b92ecf707a50b1c2c5', '8922a988522761e78f0444350218e73b', '873eb5f3b8cc503730e472a14cd26616']\n",
      "19576          cama_mesa_banho\n",
      "21153       relogios_presentes\n",
      "44653         moveis_decoracao\n",
      "44654         moveis_decoracao\n",
      "44655         moveis_decoracao\n",
      "44656         moveis_decoracao\n",
      "72765    utilidades_domesticas\n",
      "72766    utilidades_domesticas\n",
      "72767    utilidades_domesticas\n",
      "93217          cama_mesa_banho\n",
      "Name: product_category_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_customer = \"6c8a03b35eb1de3c0012232b0ff0522d\"\n",
    "recommended = recommend_products(test_customer)\n",
    "print(recommended)\n",
    "\n",
    "# remove any duplicates from the recommended list\n",
    "recommended = list(set(recommended))\n",
    "\n",
    "filtered_df = data[data['product_id'].isin(recommended)]\n",
    "\n",
    "# select the product category name column\n",
    "product_category_names = filtered_df['product_category_name']\n",
    "\n",
    "# print the product category names\n",
    "print(product_category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ciii\"></a>\n",
    "## c.iii) Collaborative Filtering Conclusion\n",
    "\n",
    "In conclusion, Collaborative Filtering is a machine learning model used for recommending items to users based on their previous purchase history and other users' purchases. The model works by identifying patterns in customer orders and recommending products to customers based on the previous customers' orders. By inputting a customer ID into the model, the output is a list of recommended products for the user. Our model's success is indicated by the list of recommended product IDs, which correctly recommend similar products to the customers' previous purchases. Overall, the Collaborative Filtering model was a useful tool for providing personalised recommendations to customers, which could improve their shopping experience and boost sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_[Go to top](#top-of-page)_\n",
    "\n",
    "<a id=\"d\"></a>\n",
    "# d) KNN with Means Model\n",
    "We will be using item-based collaborative filtering, a technique used by companies such as Amazon. This type of filtering looks at similar items based on the items a customer has already purchased or interacted with (Qutbuddin, 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"di\"></a>\n",
    "## d.i) KNN with Means Model (with Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split # note: replaces the import from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 172.4513\n"
     ]
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "data = pd.merge(orders, order_items, on=\"order_id\")\n",
    "\n",
    "# relevant columns for the user-product matrix\n",
    "data = data[[\"customer_id\", \"product_id\", \"price\"]]\n",
    "\n",
    "reader = Reader(rating_scale=(data[\"price\"].min(), data[\"price\"].max()))\n",
    "\n",
    "data = Dataset.load_from_df(data, reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Define the item-based collaborative filtering algorithm\n",
    "algo = KNNWithMeans(sim_options={\"name\": \"cosine\", \"user_based\": False})\n",
    "\n",
    "algo.fit(trainset)\n",
    "\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate the performance using root mean squared error\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised RMSE with Price: 0.02560847147928271\n"
     ]
    }
   ],
   "source": [
    "# Calculate the normalized RMSE by dividing it by the range of values in the dataset\n",
    "min_value = data.df[\"price\"].min()\n",
    "max_value = data.df[\"price\"].max()\n",
    "normalised_rmse = rmse / (max_value - min_value)\n",
    "\n",
    "# Print the normalized RMSE value\n",
    "print(\"Normalised RMSE with Price:\", normalised_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error of the training is a value between 0 and 1. A score of 0 indictate a good fitting model, and 1 represents a poorly fitting model (Zach, 2021).\n",
    "The RMSE score for just the price seems to be a good model for product recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dii\"></a>\n",
    "## d.ii) KNN with Means Model (with Price and Review Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a product price and review score in the model will help to recommend products which are rated highly by other customers. By using feature engineering, we can create a new variable for training which can help improve accuracy (Patel, 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 772.5150\n"
     ]
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "data = pd.merge(orders, order_items, on=\"order_id\")\n",
    "data = pd.merge(data, reviews, on=\"order_id\")\n",
    "\n",
    "# relevant columns for the user-product matrix\n",
    "data = data[[\"customer_id\", \"product_id\", \"price\", \"review_score\"]]\n",
    "\n",
    "# Use feature engineering to combine price and review_score\n",
    "data[\"combined_rating\"] = data[\"price\"] * data[\"review_score\"]\n",
    "\n",
    "reader = Reader(rating_scale=(data[\"combined_rating\"].min(), data[\"combined_rating\"].max()))\n",
    "\n",
    "data = Dataset.load_from_df(data[[\"customer_id\", \"product_id\", \"combined_rating\"]], reader)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Define the item-based collaborative filtering algorithm\n",
    "algo = KNNWithMeans(sim_options={\"name\": \"cosine\", \"user_based\": False})\n",
    "\n",
    "algo.fit(trainset)\n",
    "\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate the performance using root mean squared error\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised RMSE with Price and Review: 0.022940891281097314\n"
     ]
    }
   ],
   "source": [
    "# Calculate the normalized RMSE by dividing it by the range of values in the dataset\n",
    "min_value = data.df[\"combined_rating\"].min()\n",
    "max_value = data.df[\"combined_rating\"].max()\n",
    "normalised_rmse = rmse / (max_value - min_value)\n",
    "\n",
    "# Print the normalized RMSE value\n",
    "print(\"Normalised RMSE with Price and Review:\", normalised_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE score for price and review score seems to be a good model for product recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"diii\"></a>\n",
    "## d.iii) KNN with Means Model - Product Recommendations for a User ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top products for user: 4e7b3e00288586ebd08712fdd0374a03\n",
      "bd07b66896d6f1494f5b86251848ced7 - Category: moveis_escritorio\n"
     ]
    }
   ],
   "source": [
    "# Choose a specific user to make recommendations for\n",
    "user_id = \"4e7b3e00288586ebd08712fdd0374a03\"\n",
    "\n",
    "user_unrated_products = data.df.loc[data.df[\"customer_id\"] == user_id, \"product_id\"].unique()\n",
    "\n",
    "# Predict using the trained model\n",
    "user_predictions = [algo.predict(user_id, product_id) for product_id in user_unrated_products]\n",
    "\n",
    "# Sort the predictions\n",
    "user_predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "\n",
    "# Get the top products\n",
    "top_products = [pred.iid for pred in user_predictions[:10]]\n",
    "\n",
    "# Print the top 10 products for the user with their category names\n",
    "print(\"Top products for user:\", user_id)\n",
    "for product in top_products:\n",
    "    print(product, \"- Category:\", products.loc[products[\"product_id\"] == product, \"product_category_name\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to take a User ID as input, and output a list of products recommended to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"div\"></a>\n",
    "## d.iv) KNN with Means Conclusion ##\n",
    "\n",
    "In conclusion, KNN with means is a collaborative filtering algorithm that makes recommendations by finding the K-nearest neighbours to a given user and then takes the mean rating of those neighbours to predict a rating for a specific item. The benefits of KNN with means include its simplicity, efficiency, and ability to handle large datasets with high sparsity. Additionally, KNN with means is highly interpretable, as the recommendations it provides are based on the ratings of similar users.\n",
    "\n",
    "In our recommender system, we have utilised the KNN with means method. Using the price parameter we achieved a RMSE of 0.025, but adding the review score as well (using feature engineering) we managed to improve the RMSE to 0.021. We have managed to improve the prediction accuracy of the product recommendations to a high level of prediction accuracy. By using a KNN with Means model, we have used item-based collaborative filtering. We have managed to improve the prediction accuracy of the product recommendations to a high level of prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Subtask 3 & 4: Conclusion #\n",
    "The KNN model produced good results. A precision of greater than 70% is deemed a good model. We managed to achieve 77% and 87% precision. The RFC model seemed to over-fit and produced a precision score of 97%. We recommend not using the RFC model.\n",
    "\n",
    "Collaborative filtering makes more sense abstractly (recommend products based on other users purchasing trends), and in practice this produced promising results. The Product ID's which were recommended for a given user related to the purchases by other uses.\n",
    "\n",
    "By using item-based collaborative filtering (using KNN with Means) we have managed to create a highly accurate product recommendation system.\n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. References #\n",
    "\n",
    "Chandana, D. (2021) Exploring Customers Segmentation With RFM Analysis and K-Means Clustering. Available from: https://medium.com/web-mining-is688-spring-2021/exploring-customers-segmentation-with-rfm-analysis-and-k-means-clustering-118f9ffcd9f0 [Accessed 3 March 2023].\n",
    "\n",
    "Google Machine Learning (2023) Collaborative Filtering. Available from: https://developers.google.com/machine-learning/recommendation/collaborative/basics [Accessed 5 March 2023].\n",
    "\n",
    "LEDU (2018) Understanding K-means Clustering in Machine Learning. Available at: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1 [Accessed 5 March 2023].\n",
    "\n",
    "Patel, H. (2021) What is Feature Engineering â€” Importance, Tools and Techniques for Machine Learning. Available from: https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10 [Accessed 7 March 2023].\n",
    "\n",
    "Qutbuddin, M. (2020) Comprehensive Guide on Item Based Collaborative Filtering. Available from: https://towardsdatascience.com/comprehensive-guide-on-item-based-recommendation-systems-d67e40e2b75d [Accessed 7 March 2023].\n",
    "\n",
    "Sruthi, E. R. (2021) Understand Random Forest Algorithms With Examples. Available from: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ [Accessed 5 March 2023].\n",
    "\n",
    "Zack (2021) What is Considered a Good RMSE Value?. Available from: https://www.statology.org/what-is-a-good-rmse/ [Accessed 7 March 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of subtask 3 and 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
