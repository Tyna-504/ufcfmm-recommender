{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top-of-page\"></a>\n",
    "# Table of Contents #\n",
    "Click on a chapter:<br>\n",
    "**[1. Training and Testing the Models](#subtask3)<br>**\n",
    "[a. K-Nearest Neighbour](#1a-KNN)<br>\n",
    "[b. KNN First Model Findings](#1b-findings)<br>\n",
    "[c. KNN Conclusion](#1c-conclusion)<br>\n",
    "[d. Random Forest Classifier](#1d-RFC)<br>\n",
    "[e. Conclusion](#1e-conclusion)<br>\n",
    "**[2. Collaborative Filtering](#2-collaborative-filtering)<br>**\n",
    "[a. Collaborative Filtering Conclusion](#2b-conclusion)<br>\n",
    "**[3. Conclusions](#3_conclusion)<br>**\n",
    "**[4. References](#4_references)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtasks 3 and 4 are closely intertwined, with the former involving building the recommendation system and the latter involving evaluating its effectiveness using appropriate metrics. Performing these tasks together in one notebook allows for a more seamless and integrated approach to building and evaluating the recommendation system. This also makes it easier to keep track of the steps taken and results obtained and make any necessary changes or improvements. Additionally, since these tasks are shorter compared to Subtasks 1, 2, and 5, it makes sense to keep them together in one notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subtask3\"></a>\n",
    "# 1. Subtask 3 & 4: Training and Testing the models\n",
    "\n",
    "To train the model, we will select features such as product price and review score and use the target variable of the product ID as the prediction output.\n",
    "\n",
    "There are many different machine learning models available to use. From our research, we have discovered the K-Neighbours (K-means) model, random forest classifier model, and collaborative filtering are the most suitable for our dataset based off of research into other projects using a customer e-commerce dataset.\n",
    "\n",
    "To begin with, we will create models using KNeighborsClassifier and RandomForestClassifier. \n",
    "\n",
    "\n",
    "It is crucial to evaluate the effectiveness of the recommender system to ensure that it is performing well and providing accurate recommendations to users. The metrics such as precision, recall, f1, etc., are commonly used to evaluate the performance of recommendation systems.<br>\n",
    "\n",
    "However, the choice of evaluation metric should depend on the specific goals of the recommendation system and the type of data being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1a-KNN\"></a>\n",
    "## a) KNN (K-Nearest Neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbour is a supervised clustering algorithm which groups similar data points together (LEDU, 2018). KNN is one of the more simpler models to implement, but can still produce meaningful results. Clusters of similar customers can be created, based on their data parameters we can use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared and split the data into train and test data, we can train the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the KNN model\n",
      "Precision: 0.10278467373385747\n",
      "Recall: 0.13081844253253916\n",
      "f1: 0.10201569046036013\n"
     ]
    }
   ],
   "source": [
    "# create and train the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data using the KNN model\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1b-findings\"></a>\n",
    "## b) KNN First Model Findings\n",
    "Our results for the first run of KNN are unusual. A low score of 10% precision and F1 score are not great. \n",
    "\n",
    "To try and improve the accuracy and precision of our model, we will implement one-hot encoding for categorical data, scale the variable to normalise their values and remove bias, and use grid search to determine the best K value for KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the best KNN model\n",
      "Precision: 0.2600255992782448\n",
      "Recall: 0.3117283950617284\n",
      "f1: 0.27288607658894076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d9d31c22c83b>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score', 'product_category_name', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']]\n",
    "y = data['product_id']\n",
    "\n",
    "# encode the categorical feature as numeric\n",
    "X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
    "\n",
    "# scale the features using standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# create a parameter grid for k\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "\n",
    "# create and train the model using grid search and cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # print the best value of k\n",
    "# print(\"Best value of k:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on the testing data using the best KNN model\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"Scores for the best KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1c-conclusion\"></a>\n",
    "## c) KNN Conclusion\n",
    "\n",
    "We have concluded that due to the low scores of the KNN model, our data is not suitable for the KNN model. After standardising the data, removing N/A values, testing different K values, nothing we did produced a high enough score (>60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1d-RFC\"></a>\n",
    "## d) RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier is a supervised clustering algorithm for classification and regression problems (Sruthi, 2021). RFC can deal with categorical data and outliers easily which makes it a good choice for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, n_estimators=10, n_jobs=-1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, n_jobs=-1, max_depth=4, verbose=0)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the system we are running the notebook on, we had to reduce the size of the dataset in order to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: JupyterHub kernel crashes due to the size of the data, therefore we have grabbed a small sample of test data.\n",
    "X_test_rfc = X_test.sample(frac=0.3, random_state=200)\n",
    "y_test_rfc = y_test.sample(frac=0.3, random_state=200)\n",
    "\n",
    "# make predictions on the testing data using the RFC model\n",
    "y_pred_rfc = rfc.predict(X_test_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the RFC model\n",
      "Precision: 0.002319145428238781\n",
      "Recall: 0.3117283950617284\n",
      "f1: 0.27288607658894076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test_rfc, y_pred_rfc, average=\"weighted\")\n",
    "# recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "# f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the RFC model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1e-conclusion\"></a>\n",
    "## e) Conclusion\n",
    "\n",
    "The results for the RFC model is poor. Again, this could be due to similar reasons as the KNN model. The dataset can be quite sparse, and not have much distance between the 'clusters'. \n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-collaborative-filtering\"></a>\n",
    "# 2. Collaborative Filtering\n",
    "\n",
    "\n",
    "Collaborative Filtering is a model creating for recommending an item to a user, based off of their previous purchased items and other users purchases (Google Machine Learning, 2023). \n",
    "\n",
    "If user A buys an item, and user B has a similar purchase history, user B will be recommended the new item too.\n",
    "\n",
    "Collaborative filtering takes customers' previous orders and identifies patterns, and recommends products to customers based on previous customers' orders. If customer A orders products X and Y, then customer B who has ordered product X, will be recommended product Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "\n",
    "# filter the data to include only the most active customers\n",
    "# NOTE: this is mainly done due to performance issues.\n",
    "customer_counts = data['customer_id'].value_counts()\n",
    "active_customers = customer_counts[customer_counts > 3].index\n",
    "data = data[data['customer_id'].isin(active_customers)]\n",
    "\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# create a pivot table with customers as rows and products as columns using the training data\n",
    "pivot = train_data.pivot_table(index='customer_id', columns='product_id', values='review_score')\n",
    "\n",
    "# fill missing values with 0\n",
    "pivot = pivot.fillna(0)\n",
    "\n",
    "# convert the pivot table to a sparse matrix\n",
    "matrix = csr_matrix(pivot.values)\n",
    "\n",
    "# create and fit a NearestNeighbors model using the training data\n",
    "model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to recommend products for a given customer\n",
    "def recommend_products(customer_id):\n",
    "    # find the index of the customer in the pivot table\n",
    "    customer_index = pivot.index.get_loc(customer_id)\n",
    "    \n",
    "    # find the k nearest neighbors of the customer\n",
    "    distances, indices = model.kneighbors(pivot.iloc[customer_index, :].values.reshape(1, -1), n_neighbors=6)\n",
    "    \n",
    "    # get the product ids of the products purchased by the nearest neighbors\n",
    "    product_ids = []\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "#             product_ids.extend(pivot.index[indices.flatten()[i]])\n",
    "            # append the recommended product ID to the array.\n",
    "            product_ids.append(pivot.columns[indices.flatten()[i]])\n",
    "    \n",
    "    # return the most common product ids\n",
    "    return pd.Series(product_ids).value_counts().head().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86f2416d4670e4ea3ca5494d043d9f24', '86b22a03cb72239dd53996a67df35c63', '8509049c56caff468e3f35c4eefb6035', '86f024d3bdcdb9b54c9fffd92be39f54', '870bcc6c58e03ca658cfdd13db4bbe28']\n"
     ]
    }
   ],
   "source": [
    "# test the recommend_products function on a customer from the testing data.\n",
    "test_customer = test_data.iloc[0]['customer_id']\n",
    "recommended_products = recommend_products(test_customer)\n",
    "print(recommended_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2b-conclusion\"></a>\n",
    "## b) Collaborative Filtering Conclusion\n",
    "The collaborative filtering model has successfully generated product ID's to recommend to customers (based on similar customers purchasing history)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_conclusion\"></a>\n",
    "# 3. Subtask 3 & 4: Conclusion\n",
    "The KNN model and RFC model produced poor results. Ideally we would have liked 70%+ in accuracy and precision, however, the maximum score we managed to achieve using these models was 40%.\n",
    "\n",
    "Collaborative filtering makes more sense abstractly (recommend products based on other users purchasing trends), and in practice this produced promising results. The Product ID's which were recommended for a given user related to the purchases by other uses.\n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running the Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing a customer ID into the recommender model, the output is a list of recommended products for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['86f024d3bdcdb9b54c9fffd92be39f54', '86b22a03cb72239dd53996a67df35c63', '8509049c56caff468e3f35c4eefb6035', '86f2416d4670e4ea3ca5494d043d9f24', '84f47b7ffbd21c845d197cc0a7bc479a']\n",
      "19511                     moveis_decoracao\n",
      "19512                     moveis_decoracao\n",
      "80814                          moveis_sala\n",
      "80815                          moveis_sala\n",
      "89934    construcao_ferramentas_construcao\n",
      "89935    construcao_ferramentas_construcao\n",
      "89936    construcao_ferramentas_construcao\n",
      "89937    construcao_ferramentas_construcao\n",
      "91478                    moveis_escritorio\n",
      "91479                    moveis_escritorio\n",
      "99232                utilidades_domesticas\n",
      "99233                utilidades_domesticas\n",
      "99234                utilidades_domesticas\n",
      "99235                utilidades_domesticas\n",
      "99236                utilidades_domesticas\n",
      "99237                utilidades_domesticas\n",
      "Name: product_category_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_customer = \"6c8a03b35eb1de3c0012232b0ff0522d\"\n",
    "recommended = recommend_products(test_customer)\n",
    "print(recommended)\n",
    "\n",
    "# remove any duplicates from the recommended list\n",
    "recommended = list(set(recommended))\n",
    "\n",
    "filtered_df = data[data['product_id'].isin(recommended)]\n",
    "\n",
    "# select the product category name column\n",
    "product_category_names = filtered_df['product_category_name']\n",
    "\n",
    "# print the product category names\n",
    "print(product_category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4_references\"></a>\n",
    "## References\n",
    "\n",
    "Chandana, D. (2021) Exploring Customers Segmentation With RFM Analysis and K-Means Clustering. Available from: https://medium.com/web-mining-is688-spring-2021/exploring-customers-segmentation-with-rfm-analysis-and-k-means-clustering-118f9ffcd9f0 [Accessed 3 March 2023].\n",
    "\n",
    "Google Machine Learning (2023) Collaborative Filtering. Available from: https://developers.google.com/machine-learning/recommendation/collaborative/basics [Accessed 5 March 2023].\n",
    "\n",
    "LEDU (2018) Understanding K-means Clustering in Machine Learning. Available at: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1 [Accessed 5 March 2023].\n",
    "\n",
    "Sruthi, E. R. (2021) Understand Random Forest Algorithms With Examples. Available from: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ [Accessed 5 March 2023].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*End of subtask 3 and 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMenv",
   "language": "python",
   "name": "dmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
