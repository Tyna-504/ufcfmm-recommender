{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top-of-page\"></a>\n",
    "# Table of Contents #\n",
    "Click on a chapter:<br>\n",
    "**[1. Training and Testing the Models](#subtask3)<br>**\n",
    "[a. K-Nearest Neighbour](#1a-KNN)<br>\n",
    "[b. KNN First Model Findings](#1b-findings)<br>\n",
    "[c. KNN Conclusion](#1c-conclusion)<br>\n",
    "[d. Random Forest Classifier](#1d-RFC)<br>\n",
    "[e. Conclusion](#1e-conclusion)<br>\n",
    "**[2. Collaborative Filtering](#2-collaborative-filtering)<br>**\n",
    "[a. Collaborative Filtering Conclusion](#2b-conclusion)<br>\n",
    "**[3. Conclusions](#3_conclusion)<br>**\n",
    "**[4. References](#4_references)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtasks 3 and 4 are closely intertwined, with the former involving building the recommendation system and the latter involving evaluating its effectiveness using appropriate metrics. Performing these tasks together in one notebook allows for a more seamless and integrated approach to building and evaluating the recommendation system. This also makes it easier to keep track of the steps taken and results obtained and make any necessary changes or improvements. Additionally, since these tasks are shorter compared to Subtasks 1, 2, and 5, it makes sense to keep them together in one notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subtask3\"></a>\n",
    "# 1. Subtask 3 & 4: Training and Testing the models\n",
    "\n",
    "To train the model, we will select features such as product price and review score and use the target variable of the product ID as the prediction output.\n",
    "\n",
    "There are many different machine learning models available to use. From our research, we have discovered the K-Neighbours (K-means) model, random forest classifier model, and collaborative filtering are the most suitable for our dataset based off of research into other projects using a customer e-commerce dataset.\n",
    "\n",
    "To begin with, we will create models using KNeighborsClassifier and RandomForestClassifier. \n",
    "\n",
    "\n",
    "It is crucial to evaluate the effectiveness of the recommender system to ensure that it is performing well and providing accurate recommendations to users. The metrics such as precision, recall, f1, etc., are commonly used to evaluate the performance of recommendation systems.<br>\n",
    "\n",
    "However, the choice of evaluation metric should depend on the specific goals of the recommendation system and the type of data being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1a-KNN\"></a>\n",
    "## a) KNN (K-Nearest Neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbour is a supervised clustering algorithm which groups similar data points together (LEDU, 2018). KNN is one of the more simpler models to implement, but can still produce meaningful results. Clusters of similar customers can be created, based on their data parameters we can use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared and split the data into train and test data, we can train the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the KNN model\n",
      "Precision: 0.10305261884642455\n",
      "Recall: 0.13183322303110523\n",
      "f1: 0.10318356126768895\n"
     ]
    }
   ],
   "source": [
    "# create and train the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data using the KNN model\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1b-findings\"></a>\n",
    "## b) KNN First Model Findings\n",
    "Our results for the first run of KNN are unusual. A low score of 10% precision and an F1 score are not great.\n",
    "\n",
    "To improve the accuracy and precision of our model, we will implement one-hot encoding for categorical data, scale the variable to normalise their values and remove bias, and use grid search to determine the best K value for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the best KNN model\n",
      "Precision: 0.2543576006188829\n",
      "Recall: 0.30599647266313934\n",
      "f1: 0.26738167971926713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d9d31c22c83b>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
      "<ipython-input-4-d9d31c22c83b>:19: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score', 'product_category_name', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']]\n",
    "y = data['product_id']\n",
    "\n",
    "# encode the categorical feature as numeric\n",
    "X.loc[:, 'product_category_name'] = X['product_category_name'].astype('category').cat.codes\n",
    "\n",
    "# scale the features using standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# create a parameter grid for k\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "\n",
    "# create and train the model using grid search and cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # print the best value of k\n",
    "# print(\"Best value of k:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on the testing data using the best KNN model\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# calculate precision, recall, f1-score\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"Scores for the best KNN model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1c-conclusion\"></a>\n",
    "## c) KNN Conclusion\n",
    "In conclusion, the K-Nearest Neighbors (KNN) model is a supervised clustering algorithm that groups similar data points. Although it is one of the simpler models to implement, it can still produce meaningful results. In the first run, our KNN model produced low scores of precision and F1, indicating the need for improvement. To increase the accuracy and precision of the model, we implemented one-hot encoding for categorical data, normalised the variables to remove bias, and used grid search to determine the best K value for KNN. Despite our efforts, the best KNN model still produced scores lower than 30%, indicating that our data may not be suitable for KNN modelling. A score of at least 60% would be considered high enough. Nonetheless, KNN is a helpful algorithm for clustering similar data points together, and its implementation is worth considering for datasets with more suitable features and target variables.<br>\n",
    "\n",
    "As part of our investigation into improving the performance of our model, we decided to explore the Random Forest Classifier (RFC) algorithm. Our rationale behind this decision was to achieve higher scores than those previously obtained using the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1d-RFC\"></a>\n",
    "## d) RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier is a supervised clustering algorithm for classification and regression problems (Sruthi, 2021). RFC can deal with categorical data and outliers easily which makes it a good choice for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, n_estimators=10, n_jobs=-1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "# select the features and target variable\n",
    "X = data[['price', 'review_score']]\n",
    "y = data['product_id']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, n_jobs=-1, max_depth=4, verbose=0)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to the system we are running the notebook on, we had to reduce the size of the dataset in order to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: JupyterHub kernel crashes due to the size of the data, therefore we have grabbed a small sample of test data.\n",
    "X_test_rfc = X_test.sample(frac=0.3, random_state=200)\n",
    "y_test_rfc = y_test.sample(frac=0.3, random_state=200)\n",
    "\n",
    "# make predictions on the testing data using the RFC model\n",
    "y_pred_rfc = rfc.predict(X_test_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for the RFC model\n",
      "Precision: 0.002782933393476902\n",
      "Recall: 0.30599647266313934\n",
      "f1: 0.26738167971926713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall, f1-score, and AUC\n",
    "precision = precision_score(y_test_rfc, y_pred_rfc, average=\"weighted\")\n",
    "# recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "# f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Scores for the RFC model\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1e-conclusion\"></a>\n",
    "## e) Conclusion\n",
    "In conclusion, we tried the RFC method to improve the scores of our dataset. RFC is a supervised clustering algorithm capable of handling categorical data and outliers. We merged several datasets, selected the features and target variables, and split the data into training and testing sets. We then fit the RFC model on the training data, made predictions on the testing data, and calculated the precision score. Unfortunately, the RFC model produced poor results, with a precision score of only 0.0028. This could be due to the sparse nature of the dataset or the need for more distance between clusters. <br>\n",
    "\n",
    "Given these results, we have decided to proceed with Collaborative Filtering as our next analysis method.\n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-collaborative-filtering\"></a>\n",
    "# 2. Collaborative Filtering\n",
    "\n",
    "\n",
    "Collaborative Filtering is a model creating for recommending an item to a user, based off of their previous purchased items and other users purchases (Google Machine Learning, 2023). \n",
    "\n",
    "If user A buys an item, and user B has a similar purchase history, user B will be recommended the new item too.\n",
    "\n",
    "Collaborative filtering takes customers' previous orders and identifies patterns, and recommends products to customers based on previous customers' orders. If customer A orders products X and Y, then customer B who has ordered product X, will be recommended product Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data from the CSV files\n",
    "orders = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "products = pd.read_csv('data/olist_products_dataset.csv')\n",
    "reviews = pd.read_csv('data/olist_order_reviews_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# merge the datasets to create a single DataFrame\n",
    "data = pd.merge(orders, order_items, on='order_id')\n",
    "data = pd.merge(data, products, on='product_id')\n",
    "data = pd.merge(data, reviews, on='order_id')\n",
    "\n",
    "\n",
    "# filter the data to include only the most active customers\n",
    "# NOTE: this is mainly done due to performance issues.\n",
    "customer_counts = data['customer_id'].value_counts()\n",
    "active_customers = customer_counts[customer_counts > 3].index\n",
    "data = data[data['customer_id'].isin(active_customers)]\n",
    "\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# create a pivot table with customers as rows and products as columns using the training data\n",
    "pivot = train_data.pivot_table(index='customer_id', columns='product_id', values='review_score')\n",
    "\n",
    "# fill missing values with 0\n",
    "pivot = pivot.fillna(0)\n",
    "\n",
    "# convert the pivot table to a sparse matrix\n",
    "matrix = csr_matrix(pivot.values)\n",
    "\n",
    "# create and fit a NearestNeighbors model using the training data\n",
    "model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to recommend products for a given customer\n",
    "def recommend_products(customer_id):\n",
    "    # find the index of the customer in the pivot table\n",
    "    customer_index = pivot.index.get_loc(customer_id)\n",
    "    \n",
    "    # find the k nearest neighbors of the customer\n",
    "    distances, indices = model.kneighbors(pivot.iloc[customer_index, :].values.reshape(1, -1), n_neighbors=6)\n",
    "    \n",
    "    # get the product ids of the products purchased by the nearest neighbors\n",
    "    product_ids = []\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "#             product_ids.extend(pivot.index[indices.flatten()[i]])\n",
    "            # append the recommended product ID to the array.\n",
    "            product_ids.append(pivot.columns[indices.flatten()[i]])\n",
    "    \n",
    "    # return the most common product ids\n",
    "    return pd.Series(product_ids).value_counts().head().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['38cd38029795797c97b73421fdad08cf', '8973d773c115b9e347c34a248f17bc92', '88c20c5a22f2ca169af8cfc2df00a7a2', '892bc3e900a6ad3cba5112ccdb33466f', '87689c3ea34514e449355126a5fc299e']\n"
     ]
    }
   ],
   "source": [
    "# test the recommend_products function on a customer from the testing data.\n",
    "test_customer = test_data.iloc[0]['customer_id']\n",
    "recommended_products = recommend_products(test_customer)\n",
    "print(recommended_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2b-conclusion\"></a>\n",
    "## b) Collaborative Filtering Conclusion\n",
    "In conclusion, in the case of the Olist datasets, Collaborative Filtering is an effective model for recommending products to customers based on their purchasing history and the purchasing history of similar customers. By analysing patterns in previous orders, Collaborative Filtering can generate recommendations for products that a customer is likely to be interested in. The model has been successfully implemented, and the function to recommend products for a given customer has generated product IDs for recommended products based on the purchasing history of similar customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_conclusion\"></a>\n",
    "# 3. Subtask 3 & 4: Conclusion\n",
    "In conclusion, we explored different machine learning models, such as KNN, RFC, and Collaborative Filtering, and evaluated them using appropriate metrics such as precision, recall, and f1 score. After standardising the data, removing N/A values, and testing different K values, the KNN and RFC models did not produce satisfactory results, with a maximum accuracy and precision score of 40%. \n",
    "\n",
    "However, Collaborative Filtering proved to be an effective model for recommending products to customers based on their purchasing history and the purchasing history of similar customers. Analysing patterns in previous orders generated recommendations for products that a customer will likely be interested in. The function to recommend products for a given customer generated product IDs for recommended products based on the purchasing history of similar customers. \n",
    "\n",
    "Ultimately, evaluating different models helped us select the most suitable recommendation system for our dataset.\n",
    "\n",
    "<br>_[Go to top](#top-of-page)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4_references\"></a>\n",
    "## References\n",
    "\n",
    "Chandana, D. (2021) Exploring Customers Segmentation With RFM Analysis and K-Means Clustering. Available from: https://medium.com/web-mining-is688-spring-2021/exploring-customers-segmentation-with-rfm-analysis-and-k-means-clustering-118f9ffcd9f0 [Accessed 3 March 2023].\n",
    "\n",
    "Google Machine Learning (2023) Collaborative Filtering. Available from: https://developers.google.com/machine-learning/recommendation/collaborative/basics [Accessed 5 March 2023].\n",
    "\n",
    "LEDU (2018) Understanding K-means Clustering in Machine Learning. Available at: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1 [Accessed 5 March 2023].\n",
    "\n",
    "Sruthi, E. R. (2021) Understand Random Forest Algorithms With Examples. Available from: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ [Accessed 5 March 2023].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*End of subtask 3 and 4*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMenv",
   "language": "python",
   "name": "dmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
